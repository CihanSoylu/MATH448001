{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Project-2-Linear-Regression.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"2nS8-e033n6B","colab_type":"text"},"source":["# Project 2\n","\n","In this project you will be implementing the linear regression algorithm.\n","\n","Run the following code cell to import the necessary utility and test functions."]},{"cell_type":"code","metadata":{"id":"h-PShFsp3HxL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":153},"outputId":"434534d2-2e15-47d7-ecac-d62f9bcedb73","executionInfo":{"status":"ok","timestamp":1580926812693,"user_tz":300,"elapsed":10915,"user":{"displayName":"Cihan Soylu","photoUrl":"https://lh4.googleusercontent.com/-i3-9q-bZNSc/AAAAAAAAAAI/AAAAAAAAAng/7qvvlFUHucw/s64/photo.jpg","userId":"04575478996568472015"}}},"source":["import os\n","%tensorflow_version 2.x \n","import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","if not os.path.isdir('MATH448001'):\n","    !git clone https://github.com/CihanSoylu/MATH448001.git\n","\n","from MATH448001.project_tests import project_2_tests\n","from MATH448001.project_utils import project_2_utils"],"execution_count":1,"outputs":[{"output_type":"stream","text":["TensorFlow 2.x selected.\n","Cloning into 'MATH448001'...\n","remote: Enumerating objects: 133, done.\u001b[K\n","remote: Counting objects: 100% (133/133), done.\u001b[K\n","remote: Compressing objects: 100% (86/86), done.\u001b[K\n","remote: Total 133 (delta 79), reused 95 (delta 43), pack-reused 0\u001b[K\n","Receiving objects: 100% (133/133), 22.75 KiB | 4.55 MiB/s, done.\n","Resolving deltas: 100% (79/79), done.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"afQoEG0TiEo9","colab_type":"text"},"source":["# Exercise 1 (10 points)\n","\n","The weights of a neural network are randomly initialized in the beginning of training. There are different ways to initialize the weights. One popular method is called `GlorotUniform` which initializes a weight matrix of shape $(n_{in}, n_{out})$ by sampling from a uniform distribution over $[-l, l]$ where \n","$$\n","l = \\sqrt{\\frac{6}{n_{in}+n_{out}}}\n","$$\n","\n","Implement a function that takes integers `n_in, n_out` as its arguments and returns a random numpy array of shape `(n_in, n_out)` sampled from the uniform distribution above. \n","\n","The biases are assigned to zero matrix of shape `(1, n_out)`. \n","\n","Hint: [`np.random.uniform`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.uniform.html)"]},{"cell_type":"code","metadata":{"id":"MXrWNvXBqa16","colab_type":"code","colab":{}},"source":["def initialize_weights_and_biases(n_in, n_out):\n","    ''' Initialize the weights and biases\n","    Arguments:\n","    layer_sizes: list of layer dimensions including input and output layers.\n","\n","    Returns:\n","    W: The weights, a numpy array of shape (n_in, n_out)\n","    b: The biases, a numpy array of shape (1, n_out)\n","    '''\n","    \n","    np.random.seed(seed = 1)\n","    \n","    ########## Your Code goes here #############\n","\n","    \n","    ############################################\n","    \n","    return W, b"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hoBwRnHWYOpt","colab_type":"code","colab":{}},"source":["project_2_tests.test_initialize_weights_and_biases(initialize_weights_and_biases)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Greut36pqDRa","colab_type":"text"},"source":["# Exercise 2 (10 points)\n","\n","**Forward pass**\n","\n","The linear regression model is shown in the following figure:\n","\n","![linear regression](https://drive.google.com/uc?export=view&id=1ylxsSWZ_qGMNtooA0-kgcRigBF-PYA1W)\n","\n","The forward pass of a given input $x = [x_1, \\ldots, x_n]$ is given by\n","$$\n","\\begin{eqnarray}\n","\\hat{y} & = & f_{w,b}(x) \\\\\n","& = & x \\cdot w + b \\\\ \n","& = & x_1w_1 + \\ldots + x_nw_n + b \\\\\n","\\end{eqnarray}\n","$$\n","\n","Implement the `forward_pass` function below that takes inputs `x`, weights and biases as its arguments and returns the prediction $\\hat{y}$. Note that your function should be able to work for multiple inputs. For example `x` can be a matrix where each row is a particular data point:\n","$$\n","X = \\begin{bmatrix} x^{(1)}_1 & \\ldots & x^{(1)}_n \\\\ \\vdots & \\ddots & \\vdots \\\\ x^{(m)}_1 & \\ldots & x^{(m)}_n\\end{bmatrix}\n","$$\n","and in this case the output should be \n","$$\n","\\hat{y} = \\begin{bmatrix} y^{(1)} \\\\ \\vdots \\\\ y^{(m)} \\end{bmatrix} = X \\cdot w + b \n","$$"]},{"cell_type":"code","metadata":{"id":"Lk53o38wPbv1","colab_type":"code","colab":{}},"source":["def forward_pass(x, weights, biases):\n","    ''' Compute the predictions for a given set of data points\n","    Arguments: \n","    x : a numpy array of shape (m, n)\n","    weights: the weight vector, a numpy array of shape (n,1)\n","    biases: the bias, a numpy array of shape (1,1)\n","\n","    Returns:\n","    y_hat: the predictions, a numpy array of shape (m,1)\n","    '''\n","    ################### Your code goes here ################\n","    \n","\n","    ########################################################"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wI35vNXfXk4U","colab_type":"code","colab":{}},"source":["project_2_tests.test_forward_pass_linear_regression(forward_pass)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CBgfGE4_alWN","colab_type":"text"},"source":["# Exercise 3 (10 points)\n","\n","In this exercise, you will implement the mean squared error function. Recall that for a given set of data points $\\mathcal{D} = \\{(x^{(i)}, y^{(i)})\\}_{1\\leq i \\leq m}$, the mean squared error as a function of the parameters $w, b$ and $\\mathcal{D}$ is defined as \n","$$\n","\\mathcal{E}(f_{w,b}, \\mathcal{D}) = \\frac{1}{m}\\sum_{i=1}^m (f_{w,b}(x^{(i)}) - y^{(i)})^2 = \\frac{1}{m}\\sum_{i=1}^m (\\hat{y}^{(i)} - y^{(i)})^2\n","$$\n","The function below takes two arguments:\n"," - `y_hat`: an array of predictions $[\\hat{y}^{(1)}, \\ldots, \\hat{y}^{(m)}]$ corresponding to the input values $[x^{(1)}, \\ldots, x^{(m)}]$. \n"," - `y`: the labels corresponding to the same input values. \n","Implement the mean_squared_error function with these two inputs. \n","\n","and returns the mean squared error computed using the definition above in terms of `y_hat` and `y`."]},{"cell_type":"code","metadata":{"id":"BrMN3aqmDn3D","colab_type":"code","colab":{}},"source":["def mean_squared_error(y_hat, y):\n","    ''' Compute mean squared error\n","    Arguments:\n","    y_hat: a numpy array of shape (m,1)\n","    y : a numpy array of shape (m,1)\n","\n","    Returns:\n","    loss: The mean squared error.\n","    '''\n","\n","    ################### Your code goes here ################\n","\n","\n","    ########################################################"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yxsjzJzY66Ga","colab_type":"code","colab":{}},"source":["project_2_tests.test_mse(mean_squared_error)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"McnPJWY7vApI","colab_type":"text"},"source":["#Exercise 4 (20 points)\n","\n","**Find gradients and update parameters**\n","\n","In this step you will implement a function which updates the weights and biases according to the gradient of the loss function calculated for a batch of $m$ data points. In machine learning lingo this step is called 'backpropagation'.\n"," \n","In order to update $w_k$, we first need to find the derivative of $\\mathcal{E}$ with respect to $w_k$:\n","$$\n","\\begin{eqnarray}\n","\\frac{\\partial \\mathcal{E}}{\\partial w_k} & = & \\frac{1}{m}\\sum_{i=1}^m\\frac{\\partial }{\\partial w_k} (\\hat{y}^{(i)} - y^{(i)})^2\\\\\n","& = & \\frac{2}{m}\\sum_{i=1}^m (\\hat{y}^{(i)} - y^{(i)}) \\frac{\\partial \\hat{y}^{(i)}}{\\partial w_k} \\\\\n","& = & \\frac{2}{m}\\sum_{i=1}^m (\\hat{y}^{(i)} - y^{(i)}) x^{(i)}_k\n","\\end{eqnarray}\n","$$\n","Similarly the derivatives with respect to $b$:\n","\\begin{eqnarray}\n","\\frac{\\partial \\mathcal{E}}{\\partial b} & = & \\frac{1}{m}\\sum_{i=1}^m\\frac{\\partial }{\\partial b} (\\hat{y}^{(i)} - y^{(i)})^2\\\\\n","& = & \\frac{2}{m}\\sum_{i=1}^m (\\hat{y}^{(i)} - y^{(i)}) \\frac{\\partial \\hat{y}^{(i)}}{\\partial b} \\\\\n","& = & \\frac{2}{m}\\sum_{i=1}^m (\\hat{y}^{(i)} - y^{(i)})\n","\\end{eqnarray}\n","\n","Now given a mini-batch $\\{(x^{(i)}, y^{(i)})\\}_{i=1}^m$, the update equations for the weights are\n","$$\n","\\begin{eqnarray}\n","w_k^{new} & = & w_k^{old} - \\alpha \\frac{\\partial \\mathcal{E}}{\\partial w_k} = w_k^{old} - \\alpha \\frac{2}{m}\\sum_{i=1}^m (\\hat{y}^{(i)} - y^{(i)}) x^{(i)}_k \\\\\n","b^{new} & = & b^{old} - \\alpha \\frac{\\partial \\mathcal{E}}{\\partial b} = b^{old} - \\alpha \\frac{2}{m}\\sum_{i=1}^m (\\hat{y}^{(i)} - y^{(i)})\n","\\end{eqnarray}\n","$$\n","\n","Implement a function that computes these derivatives and updates the weight and bias accordingly. The arguments of the function are `x`, `y`, `weights`, `biases`, and `learning_rate`. Here think of $x$ as\n","$$\n","x = \\begin{bmatrix} x^{(1)}_1 & \\ldots & x^{(1)}_n \\\\ \\vdots & \\ddots & \\vdots \\\\ x^{(m)}_1 & \\ldots & x^{(m)}_n\\end{bmatrix}\n","$$\n","and $y$ as \n","$$\n","y = \\begin{bmatrix} y^{(1)} \\\\ \\vdots \\\\ y^{(m)} \\end{bmatrix} \n","$$"]},{"cell_type":"code","metadata":{"id":"YEUX22bqPj17","colab_type":"code","colab":{}},"source":["def update_parameters(x, y, weights, biases, learning_rate):\n","    ''' Update parameters according to gradients.\n","    Arguments:\n","    x: A numpy array of shape (m,n) where each row corresponding to a data point\n","    y: A numpy array of shape (m,1) where each number is the label for the corresponding\n","        row in x\n","    weights: The weight vector, a numpy array of shape (n,1)\n","    biases: The bias value, a numpy array of shape (1,1)\n","\n","    Returns: \n","    weights: The updated weight vector, a numpy array of shape (n,1)\n","    biases: The updated bias value, a numpy array of shape (1,1) \n","    '''\n","    \n","    ################### Your code goes here ################\n","    \n","\n","    \n","    ########################################################\n","    \n","    return weights, biases"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YJ69EBpdm8n7","colab_type":"code","colab":{}},"source":["project_2_tests.test_update_parameters_linear_reg(update_parameters)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"473coDUlGY6F","colab_type":"text"},"source":["# Exercise 5: (0 points)\n","\n","Now let's load some data. Run the following cell to load training and test data. We will use the training data to train the model and then use the test data to test its performance. The point is that the model did not train on the test data, so the performance on the test data is an indication of the performance of the model on unseen data. \n","\n","To be able to create visualizations, the input features chosen to be one dimensional and the labels are one dimensional as well.  "]},{"cell_type":"code","metadata":{"id":"Q8vbJBGKWFCs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":279},"outputId":"4cf830bd-83ea-4554-b14c-34debdb85635","executionInfo":{"status":"ok","timestamp":1580926812902,"user_tz":300,"elapsed":6047,"user":{"displayName":"Cihan Soylu","photoUrl":"https://lh4.googleusercontent.com/-i3-9q-bZNSc/AAAAAAAAAAI/AAAAAAAAAng/7qvvlFUHucw/s64/photo.jpg","userId":"04575478996568472015"}}},"source":["x_train, y_train, x_test, y_test = project_2_utils.load_regression_data()\n","project_2_utils.plot_regression_data(x_train, y_train)\n","project_2_utils.plot_regression_data(x_test, y_test)"],"execution_count":2,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYAAAAEGCAYAAABsLkJ6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFABERHJKAUBEJKf+P1HaPVF0Q0trAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"SSi5pdeGIWKO","colab_type":"text"},"source":["Dataset consists on 100 training examples and 100 test examples."]},{"cell_type":"code","metadata":{"id":"PVXb8e4PIeZy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"04ddc6c9-20ef-4694-862c-76ab17efc661","executionInfo":{"status":"ok","timestamp":1580928020406,"user_tz":300,"elapsed":349,"user":{"displayName":"Cihan Soylu","photoUrl":"https://lh4.googleusercontent.com/-i3-9q-bZNSc/AAAAAAAAAAI/AAAAAAAAAng/7qvvlFUHucw/s64/photo.jpg","userId":"04575478996568472015"}}},"source":["print(x_train.shape)\n","print(y_train.shape)\n","print(x_test.shape)\n","print(y_test.shape)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["(100, 1)\n","(100, 1)\n","(100, 1)\n","(100, 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"l3ZAOYwTH2lm","colab_type":"text"},"source":["# Exercise 6 (30 points)\n","\n","Now it is time to train the linear regression model on this dataset. \n","\n","**Hyperparameters**\n","\n","Set hyperparameters `batch_size` and `learning_rate`. Try different values of these parameters to get a validation loss less than 0.95.\n","\n","`batch_size` is the number of training data points to be used for each gradient descent update. "]},{"cell_type":"code","metadata":{"id":"xESGWfLPDRN3","colab_type":"code","colab":{}},"source":["################### Your code goes here ################\n","batch_size = \n","learning_rate = \n","########################################################\n","\n","# Don't change the epochs\n","epochs = 100"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wDd-Kj4drCiD","colab_type":"text"},"source":["**Train**\n","\n","You are given part of the training loop below. At each epoch (the outer `for` loop), the model goes through the whole training data. At each step (the inner `for` loop), the model takes a mini-batch of training data calculate the loss and update its weights and biases using the average gradient computed on this mini-batch. After the whole training set is covered once, the epoch ends and the next epoch starts. Complete missing parts below. "]},{"cell_type":"code","metadata":{"id":"gDqY7eE54KqR","colab_type":"code","colab":{}},"source":["avg_loss = []\n","val_losses = []\n","\n","################### Your code goes here ################\n","# TODO: initialize weights and biases using the initialize_weights_and_biases\n","#       you implemented above.\n","weights, biases = \n","########################################################\n","\n","weight_bias_memory = []\n","\n","num_steps = len(x_train) // batch_size\n","\n","for epoch in range(1,epochs+1):\n","\n","    weight_bias_memory.append([float(weights), float(biases)])\n","    losses = []\n","\n","    for step in range(0,num_steps):\n","        batch_x = x_train[step*batch_size: (step+1)*batch_size] \n","        batch_y = y_train[step*batch_size: (step+1)*batch_size]\n","        \n","        ################### Your code goes here ################\n","        # TODO: Calculate the predictions of the model on batch_x\n","        y_hat = \n","\n","        # TODO: Find the mean squared error for y_hat and batch_y\n","        loss = \n","\n","        # TODO: Update the parameters. \n","        weights, biases = \n","        ########################################################\n","        \n","        losses.append(np.sqrt(loss))\n","\n","    avg_loss.append(np.mean(losses))\n","            \n","    y_hat = forward_pass(x_test, weights, biases)\n","    val_loss = np.sqrt(mean_squared_error(y_hat, y_test))\n","    val_losses.append(val_loss)\n","    \n","    print(\"epoch %i,   Validation Loss %f, Training Loss %f\" %(epoch, val_loss, np.mean(losses)))\n","\n","plt.plot(val_losses, label = \"Vlaidation Loss\")\n","plt.plot(avg_loss, label = \"Training loss\")\n","plt.ylabel('Loss')\n","plt.xlabel('Epochs')\n","plt.legend()\n","plt.title(\"Learning rate =\" + str(learning_rate) + \" Batch size =\" + str(batch_size))\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zbseUrJ6dw9L","colab_type":"text"},"source":["Let's plot the progression of the regression lines. The following function plots the lines corresponding to the weights after every 10 epochs. "]},{"cell_type":"code","metadata":{"id":"5cbteAZBLO1B","colab_type":"code","colab":{}},"source":["project_2_utils.plot_regression_lines(weight_bias_memory, x_train, y_train)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XxsJT-TxeD8E","colab_type":"text"},"source":["The line learned by the algorithm plotted with the test data. "]},{"cell_type":"code","metadata":{"id":"AkQpQ-oeQhxU","colab_type":"code","colab":{}},"source":["project_2_utils.plot_regression_line(float(weights), float(biases))\n","project_2_utils.plot_regression_data(x_test, y_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PU_SN1MpeLHw","colab_type":"text"},"source":["Values of the learned weights and biases."]},{"cell_type":"code","metadata":{"id":"H611D7TGuZx_","colab_type":"code","colab":{}},"source":["print('weights = ', weights)\n","print('biases = ', biases)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lNCgdwZpeYZe","colab_type":"text"},"source":["The following function plots the contours of the mean squared error function over the $(w,b)$ parameter space. The red dots are plotting the weight and bias values at each iteration. As seen from the plot, the points are approaching to the center of the plot which is corresponding to the parameters for the global minimum value of the error function.  "]},{"cell_type":"code","metadata":{"id":"PXFGwUYjUUGj","colab_type":"code","colab":{}},"source":["project_2_utils.plot_gradient_descent_progression(weight_bias_memory, x_train, y_train)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BiAopPeZfMsu","colab_type":"text"},"source":["# Exercise 7 (10 points)"]},{"cell_type":"markdown","metadata":{"id":"-3m1vbFxSqqP","colab_type":"text"},"source":["The error function for the linear regression problem is a quadratic function of the parameters and so it has a global minimum and no local minimums. Hence it is possible to find an explicit solution for the optimization problem. To this end, we first extend the data matrix `X` by adding a column of ones corresponding to bias parameter. So the function $f_{w,b}$ can be written as follows:\n","$$\n","\\begin{eqnarray*}\n","\\hat{y} & = & f_{w,b}(X) \\\\ \n","& = & X \\cdot w + b  \\\\\n","& = & \\begin{bmatrix}\n","x^{(1)}_1 & \\ldots & x^{(1)}_n & 1 \\\\ \\vdots & \\ddots & \\vdots & \\vdots \\\\ x^{(m)}_1 & \\ldots & x^{(m)}_n & 1\n","\\end{bmatrix}\n","\\begin{bmatrix}\n","w_1 \\\\ \\vdots \\\\ w_n \\\\ b\n","\\end{bmatrix} \\\\\n","& = & \\tilde{X} \\cdot \\tilde{w}\n","\\end{eqnarray*}\n","$$\n","For the linear system of equations $y = \\tilde{X} \\cdot \\tilde{w}$, the least square solution can be found as follows:\n","$$\n","\\tilde{w} = \\tilde{X}^+ \\cdot y\n","$$\n","where $\\tilde{X}^+$ is the Moose-Penrose pseudo inverse of the data matrix $\\tilde{X}$. \n","\n","Implement the following function which takes the data matrix `X` and the labels `y` as its arguments and returns the least square solution using the psuedo-inverse described above. \n","\n","Hint: You can use `np.append` to create $\\tilde{X}$ and `np.linalg.pinv` to find the pseudo-inverso of $\\tilde{X}$. Read the documentation of `np.linalg.pinv` function which explains how the singular value decomposition is used to compute the pseudo-inverse. "]},{"cell_type":"code","metadata":{"id":"5di_JBuqWIrB","colab_type":"code","colab":{}},"source":["def explicit_solution(X, y):\n","\n","    ################### Your code goes here ################\n","\n","\n","    \n","    ########################################################\n","\n","    return w_tilde\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"q_4urD9GV3iX","colab_type":"code","colab":{}},"source":["project_2_tests.test_explicit_solution(explicit_solution)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"46kWPwDLnVAx","colab_type":"text"},"source":["Now run this function with training data as input and compare the parameter values you get with the resulting values of the gradient descent algorithm."]},{"cell_type":"code","metadata":{"id":"QY6sVkMxV2KL","colab_type":"code","colab":{}},"source":["explicit_solution(x_train,y_train)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uHpa9kysd3B4","colab_type":"text"},"source":["# Exercise 8 (10 points)\n","\n","Let's try the algorithm you implemented on a real data set. The data is the [auto-mpg](https://archive.ics.uci.edu/ml/datasets/auto+mpg) Dataset from the [UCI Machine Learning repository](https://archive.ics.uci.edu/ml/index.php). The goal is the predict MPG (mile per gallon) from the certain features of a car. The provided features are  \n","\n","'Cylinders','Displacement','Horsepower','Weight', 'Acceleration', 'Model Year', 'Origin' and the last feature 'Origin' is a categorical feature which takes three possible values 'USA', 'Europe' or 'Japan'. One way to turn these kind of categorical features into numbers is called `One Hot Encoding`: A new binary feature added to the data corresponding to each category. So in this case the 'Origin' feature column is replaced by three feature columns for which only one of these three feature columns is 1 corresponding to the category. See the part of the dataset below for these three features. \n","\n","Run the following code cells to load the training and the test dataset and take a look at the features."]},{"cell_type":"code","metadata":{"id":"S6C8ph2JLw1i","colab_type":"code","colab":{}},"source":["df, x_train, y_train, x_test, y_test = project_2_utils.load_auto_mpg_data() "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zctv0b7GgKBx","colab_type":"code","colab":{}},"source":["df.head()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OH_m5JfbhQxF","colab_type":"text"},"source":["The dimensions of the data:"]},{"cell_type":"code","metadata":{"id":"_YJqVbgohL83","colab_type":"code","colab":{}},"source":["print(x_train.shape)\n","print(y_train.shape)\n","print(x_test.shape)\n","print(y_test.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D3JmrJTzgmDu","colab_type":"text"},"source":["Now it is time to train the linear regression model on this dataset. \n","\n","**Hyperparameters**\n","\n","Set hyperparameters `batch_size` and `learning_rate`. Try different values of these parameters to get a validation loss less than 3.5."]},{"cell_type":"code","metadata":{"id":"ghNJwDQJEQgs","colab_type":"code","colab":{}},"source":["################### Your code goes here ################\n","batch_size = \n","learning_rate = \n","########################################################\n","\n","# Do not change the epochs\n","epochs = 20"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4z6h6WaHg1r5","colab_type":"text"},"source":["**Train**\n","\n","As you did above for the artificial data set, fill in the required parts in the below training loop to train a linear regression model on Auto-MPG dataset. "]},{"cell_type":"code","metadata":{"id":"Iej90QjGDxhE","colab_type":"code","colab":{}},"source":["avg_loss = []\n","val_losses = []\n","\n","################### Your code goes here ################\n","# TODO: initialize weights and biases using the initialize_weights_and_biases\n","#       you implemented above.\n","weights, biases = \n","########################################################\n","\n","num_steps = len(x_train) // batch_size\n","\n","for epoch in range(1,epochs+1):\n","\n","    losses = []\n","\n","    for step in range(0,num_steps):\n","        batch_x = x_train[step*batch_size: (step+1)*batch_size] \n","        batch_y = y_train[step*batch_size: (step+1)*batch_size]\n","        \n","        ################### Your code goes here ################\n","        # TODO: Calculate the predictions of the model on batch_x\n","        y_hat = \n","\n","        # TODO: Find the mean squared error for y_hat and batch_y and append the  \n","        #       result to the losses list. \n","        loss = \n","\n","        # TODO: Update the parameters. \n","        weights, biases = \n","        ########################################################\n","        losses.append(np.sqrt(loss))\n","\n","    avg_loss.append(np.mean(losses))\n","            \n","    y_hat = forward_pass(x_test, weights, biases)\n","    val_loss = np.sqrt(mean_squared_error(y_hat, y_test))\n","    val_losses.append(val_loss)\n","    \n","    print(\"epoch %i,   Validation Loss %f, Training Loss %f\" %(epoch, val_loss, np.mean(losses)))\n","\n","plt.plot(val_losses, label = \"Vlaidation Loss\")\n","plt.plot(avg_loss, label = \"Training loss\")\n","plt.ylabel('Loss')\n","plt.xlabel('Epochs')\n","plt.legend()\n","plt.title(\"Learning rate =\" + str(learning_rate) + \" Batch size =\" + str(batch_size))\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gi1XmfSMEClB","colab_type":"code","colab":{}},"source":["weights, biases"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FeUBHz1DhrCW","colab_type":"text"},"source":["Let's see how the model is doing by comparing the predictions of the model and the labels for the first 10 data points in the test set. "]},{"cell_type":"code","metadata":{"id":"a5uPoDyJIQ1U","colab_type":"code","colab":{}},"source":["forward_pass(x_test[:10], weights, biases)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QwTCenivM45j","colab_type":"code","colab":{}},"source":["y_test[:10]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DCXbHaRQM59r","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}