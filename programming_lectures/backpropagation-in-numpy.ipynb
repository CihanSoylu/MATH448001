{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"backpropagation-in-numpy.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMhglcFNtqLnnW9aI3CsMWh"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"6LCeEY2hOmp4","colab_type":"text"},"source":["In this notebook, we will go over an implementation of backpropagation for a multilayer densely connected neural network using Numpy. \n","\n"," "]},{"cell_type":"code","metadata":{"id":"bYwGU_4xNfBO","colab_type":"code","outputId":"05fdc221-cc3a-4aa1-88bc-bb0ad99b7584","executionInfo":{"status":"ok","timestamp":1586886135822,"user_tz":240,"elapsed":5283,"user":{"displayName":"Cihan Soylu","photoUrl":"https://lh4.googleusercontent.com/-i3-9q-bZNSc/AAAAAAAAAAI/AAAAAAAAAng/7qvvlFUHucw/s64/photo.jpg","userId":"04575478996568472015"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["import os\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","\n","if not os.path.isdir('MATH448001'):\n","    !git clone https://github.com/CihanSoylu/MATH448001.git\n","\n","from MATH448001.project_utils import project_2_utils"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Cloning into 'MATH448001'...\n","remote: Enumerating objects: 226, done.\u001b[K\n","remote: Counting objects: 100% (226/226), done.\u001b[K\n","remote: Compressing objects: 100% (151/151), done.\u001b[K\n","remote: Total 226 (delta 135), reused 160 (delta 71), pack-reused 0\u001b[K\n","Receiving objects: 100% (226/226), 1.72 MiB | 17.63 MiB/s, done.\n","Resolving deltas: 100% (135/135), done.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"d_3w02lyPfTh","colab_type":"text"},"source":["We will denote the layer variables (e.g. weights of the layer, outputs of the layer) by an upperscript inside square brackets `[]`. For example, $W^{[k]}$ is the weight matrix of the kth layer. "]},{"cell_type":"markdown","metadata":{"id":"TNlS_CHKOBxt","colab_type":"text"},"source":["Let's start with a function that initialize the weights and biases. This is simply what you did for Project 2, now for multiple layers. The below function will return a [dictionary](https://docs.python.org/3/tutorial/datastructures.html#dictionaries) with keys `weights` and  `biases` where the values for the keys are the [lists](https://docs.python.org/3.7/tutorial/introduction.html?highlight=list#lists) of weight matrices and biases. If `layer_sizes` are $n_0, n_1, \\ldots, n_l$ where $n_0$ is the input dimension and $n_l$ is the output dimension and $l$ is the number of layers, then the weights will be a list $[W^{[1]}, W^{[2]}, \\ldots, W^{[l]}]$ where $W^{[k]}$ has dimension $n_{k-1}\\times n_{k}$ and the biases will be a list $[b^{[1]}, b^{[2]}, \\ldots, b^{[l]}]$ where $b^{[k]}$ has dimension $1\\times n_k$. \n"]},{"cell_type":"code","metadata":{"id":"-IW4mo4dNqko","colab_type":"code","colab":{}},"source":["def initialize_weights_and_biases(layer_sizes):\n","\n","  weights = []\n","  biases = []\n","  parameters = {}\n","\n","  for l in range(1, len(layer_sizes)):\n","      n_in = layer_sizes[l-1]\n","      n_out = layer_sizes[l]\n","      \n","      lim = np.sqrt(6/(n_in+n_out))\n","      weights.append(np.random.uniform(low = -lim, high = lim, size = (n_in, n_out)))\n","      biases.append(np.zeros((1,layer_sizes[l])))\n","\n","  parameters['weights'] = weights\n","  parameters['biases'] = biases\n","\n","  return parameters"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RN4xXgltQ6z5","colab_type":"text"},"source":["Let's also implement the activation functions we will need."]},{"cell_type":"code","metadata":{"id":"yri7sLbmQ0wW","colab_type":"code","colab":{}},"source":["def relu(z):\n","  \n","    activation = np.maximum(0,z)\n","        \n","    return activation\n","\n","\n","def relu_derivative(z):\n","  \n","    d_relu = np.ones(z.shape)\n","    d_relu[z < 0] = 0\n","    \n","    return d_relu\n","\n","def sigmoid(z):\n","        \n","    activation = 1/(1+np.exp(-z))\n","\n","    return activation\n","\n","def sigmoid_derivative(z):\n","    \n","    d_sigmoid = sigmoid(z) * (1-sigmoid(z))\n","\n","    return d_sigmoid"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KzGf7pawSZe7","colab_type":"text"},"source":["Now let's go over the backpropagation for a multilayer network. Recall that we need to calculate the partial derivatives of the loss function with respect to the weights and biases of each layer using chain rule. For a given data point $(x,y)$, if the model output is $\\hat{y}$, then the loss function is $L(y, \\hat{y})$, where $L$ depends on the problem type, e.g. binary-crossentropy loss for a binary classification problem. Here note that the dependence of $L(y, \\hat{y})$ to the weights and biases is through $\\hat{y}$.\n","\n","The partial derivatives $\\frac{\\partial L}{\\partial W^{[k]}}$ and $\\frac{\\partial L}{\\partial b^{[k]}}$ are \n","$$\n","\\frac{\\partial L}{\\partial W^{[k]}} = \\begin{bmatrix} \\frac{\\partial L}{\\partial W_{11}^{[k]}} & \\cdots & \\frac{\\partial L}{\\partial W_{1n_k}^{[k]}} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial L}{\\partial W_{n_{k-1}1}^{[k]}} & \\cdots & \\frac{\\partial L}{\\partial W_{n_{k-1}n_k}^{[k]}}\\end{bmatrix}\n","$$\n","and \n","$$\n","\\frac{\\partial L}{\\partial b^{[k]}} = \\begin{bmatrix}\\frac{\\partial L}{\\partial b_1^{[k]}}, \\cdots, \\frac{\\partial L}{\\partial b_{n_k}^{[k]}}  \\end{bmatrix}\n","$$\n","\n","In order to find $\\frac{\\partial L}{\\partial W_{ij}^{[k]}}$, we apply the chain rule to get\n","$$\n","\\frac{\\partial L}{\\partial W_{ij}^{[k]}} = \\frac{\\partial L}{\\partial z_{j}^{[k]}} \\frac{\\partial z_{j}^{[k]}}{\\partial W_{ij}^{[k]}}\n","$$\n","Here $z_j$ is the $j$-th component of the affine linear output of the $k$-th layer $[z_1^{[k]}, \\ldots, z_{n_k}^{[k]}]$. Considering the forward computation $z^{[k]} = h^{[k-1]}\\cdot W^{[k]} + b^{[k]}$ we see that $\\frac{\\partial z_j^{[k]}}{\\partial W_{ij}^{[k]}} = h_i^{[k-1]}$ and so \n","$$\n","\\frac{\\partial L}{\\partial W_{ij}^{[k]}} = \\frac{\\partial L}{\\partial z_{j}^{[k]}} h_i^{[k-1]}\n","$$\n","\n","This calculation, when it is done for all $i, j$, corresponds to the following matrix multiplication\n","$$\n","\\frac{\\partial L}{\\partial W^{[k]}} = {h^{[k-1]}}^T \\cdot \\frac{\\partial L}{\\partial z^{[k]}} \n","$$\n","where $\\frac{\\partial L}{\\partial z^{[k]}} = \\begin{bmatrix}\\frac{\\partial L}{\\partial z_1^{[k]}}, \\cdots, \\frac{\\partial L}{\\partial z_{n_k}^{[k]}}  \\end{bmatrix}$. Similarly \n","$$\n","\\frac{\\partial L}{\\partial b_{j}^{[k]}} = \\frac{\\partial L}{\\partial z_{j}^{[k]}} \\frac{\\partial z_{j}{[k]}}{\\partial b_{j}^{[k]}} = \\frac{\\partial L}{\\partial z_{j}^{[k]}} \\Rightarrow \\frac{\\partial L}{\\partial b^{[k]}} = \\frac{\\partial L}{\\partial z^{[k]}} \n","$$\n","\n","Finding both of the gradients, we used $\\frac{\\partial L}{\\partial z^{[k]}}$. Since $h^{[k]} = \\Phi(z^{[k]})$ where $\\Phi$ is the activation function of layer $k$, we have, by the chain rule,\n","$$\n","\\frac{\\partial L}{\\partial z^{[k]}} = \\frac{\\partial L}{\\partial h^{[k]}} \\odot \\Phi'(z^{[k]})\n","$$\n","where $\\odot$ is component-wise multiplication and $\\Phi'(z^{[k]}) = [\\Phi'(z_1^{[k]}), \\cdots, \\Phi'(z_{n_k}^{[k]})]$. We can find $\\frac{\\partial L}{\\partial h^{[k]}}$ using the relation $z^{[k+1]} = h^{[k]}\\cdot W^{[k+1]} + b^{[k+1]}$ as follows\n","$$\n","\\frac{\\partial L}{\\partial h^{[k]}} = \\frac{\\partial L}{\\partial z^{[k+1]}} \\cdot {W^{[k+1]}}^T\n","$$\n","\n","All of the above computations are for computing the gradients after a single example $ x = [x_1, \\ldots, x_{n_0}]$ in which case the linear output of layer $k$ is of the form $z^{[k]} = [z_1^{[k]}, \\ldots, z_{n_k}^{[k]} ]$. If we were to use a single example to update the weights and biases then the gradients above would be used. However in practice the gradient update is usually done after a minibatch of examples, say $m$. In that case the above computations are done for each example to calculate the gradients, then we take the average of all those gradients and this average is used for the update step. \n","\n","In the case of a minibatch of $m$ data points, the input $x$ will become a matrix\n","$$\n","x = \\begin{bmatrix} x_1^{(1)} & \\cdots & x_{n_0}^{(1)} \\\\ \\vdots & \\ddots & \\vdots \\\\ x_1^{(m)} & \\cdots & x_{n_0}^{(m)} \\end{bmatrix}\n","$$\n","Here each row corresponds to a single example. In this case the linear output of each layer will also become a matrix,\n","$$\n","z^{[k]} = \\begin{bmatrix} z_1^{(1)[k]} & \\cdots & z_{n_k}^{(1)[k]} \\\\ \\vdots & \\ddots & \\vdots \\\\ z_1^{(m)[k]} & \\cdots & z_{n_k}^{(m)[k]} \\end{bmatrix}\n","$$\n","The gradient computations take the following forms\n","$$\n","\\frac{\\partial L}{\\partial W^{[k]}} = \\frac{1}{m} \\left( {h^{[k-1]}}^T \\cdot \\frac{\\partial L}{\\partial z^{[k]}} \\right)\n","$$\n","and\n","$$\n","\\frac{\\partial L}{\\partial b^{[k]}} = \\frac{1}{m} \\left( [1, \\ldots, 1] \\cdot \\frac{\\partial L}{\\partial z^{[k]}} \\right) \n","$$\n","where the vector consisting of 1s is of dimension $1\\times m$."]},{"cell_type":"markdown","metadata":{"id":"CfjXIyLhRtmU","colab_type":"text"},"source":["Note that in the above gradient computation, the gradient with respect to the weights of layer $k$, depends on the activations of the previous layer, $h^{[k-1]}$, and the partial derivative with respect to the affine linear output $z^{[k]}$. Also note that, this partial derivative can be found by multiplying the partial derivative with respect to $h^{[k]}$ and the derivative of the activation function evaluated at $z^{[k]}$, componentwise. Because of these dependencies, during the calculation of the output of the model for a given input, we will keep the affine linear output and the activation function output of each layer to use during backpropagation."]},{"cell_type":"code","metadata":{"id":"ks2IB0pQRM_P","colab_type":"code","colab":{}},"source":["def forward_pass(data, parameters):\n","    weights = parameters['weights']\n","    biases = parameters['biases']\n","    \n","    previous_activation = data\n","    \n","    memory_buffer = []\n","    num_of_layers = len(weights)\n","    \n","    for layer in range(0, num_of_layers):\n","        \n","        z = np.dot(previous_activation, weights[layer]) + biases[layer]\n","        if layer != num_of_layers - 1:\n","            activation = relu(z)\n","        else:\n","            activation = sigmoid(z)\n","        \n","        memory = (weights[layer], biases[layer], z, previous_activation) \n","        \n","        memory_buffer.append(memory)\n","        previous_activation = activation\n","    \n","        \n","    return activation, memory_buffer"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BUFeQQ39utRz","colab_type":"text"},"source":["Binary crossentropy loss function:"]},{"cell_type":"code","metadata":{"id":"vUqCWl5Bun70","colab_type":"code","colab":{}},"source":["def compute_loss(y_hat, y):\n","    \n","    #number of examples\n","    m = y.shape[0]\n","\n","    loss = -1/m * np.sum(y * np.log(y_hat) + (1-y)*np.log(1-y_hat))\n","    \n","    return np.squeeze(loss)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C2D0pLsGu5Fq","colab_type":"text"},"source":["Backpropagation"]},{"cell_type":"code","metadata":{"id":"tsxPGzUWu6s7","colab_type":"code","colab":{}},"source":["def backpropagate(memory_buffer, y, y_hat):\n","\n","    d_loss = - (np.divide(y, y_hat) - np.divide(1 - y, 1 - y_hat))\n","    \n","    m = y.shape[0]\n","    \n","    num_layers = len(memory_buffer)\n","    gradients = [0] * num_layers    \n","        \n","    dL_da = d_loss\n","    \n","    \n","    for layer in reversed(range(0, num_layers)):\n","        w, b, z, prev_activation = memory_buffer[layer]\n","        \n","        if layer == num_layers - 1:\n","            dL_dz = dL_da * sigmoid_derivative(z)     \n","        else:\n","            dL_dz = dL_da * relu_derivative(z) \n","            \n","        dL_dw = 1/m * np.dot(prev_activation.T, dL_dz)\n","        dL_db = 1/m * np.dot(np.ones((1,m)), dL_dz)\n","        \n","        gradients[layer] = (dL_dw, dL_db)\n","                \n","        dL_da = np.dot(dL_dz, w.T)\n","        \n","    return gradients"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xRp73c8VvYGk","colab_type":"code","colab":{}},"source":["def update_parameters(gradients, parameters, learning_rate):\n","\n","    num_of_layers = len(parameters['biases'])\n","    \n","    for l in range(0,num_of_layers):\n","        parameters['weights'][l] = parameters['weights'][l] - learning_rate * gradients[l][0]\n","        parameters['biases'][l] = parameters['biases'][l] - learning_rate * gradients[l][1] \n","    return parameters"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lsNh286UxffP","colab_type":"code","colab":{}},"source":["def predict(data, parameters):\n","    \n","    y_hat, _ = forward_pass(data, parameters)\n","    \n","    return y_hat"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mjDoLHF4xxYA","colab_type":"code","colab":{}},"source":["def accuracy(y, y_hat):\n","    m = y.shape[0]\n","    pred = [int(x > 0.5) for x in y_hat]\n","    pred = np.reshape(pred, (m, -1))\n","        \n","    return np.sum((pred == y))/m"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AbE4515avpKx","colab_type":"text"},"source":["Get data"]},{"cell_type":"code","metadata":{"id":"jBP5mhprv4cH","colab_type":"code","outputId":"78f1abe7-20b3-4acb-9e88-bc36ef397917","executionInfo":{"status":"ok","timestamp":1586887312983,"user_tz":240,"elapsed":590,"user":{"displayName":"Cihan Soylu","photoUrl":"https://lh4.googleusercontent.com/-i3-9q-bZNSc/AAAAAAAAAAI/AAAAAAAAAng/7qvvlFUHucw/s64/photo.jpg","userId":"04575478996568472015"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["x_train, x_val, y_train, y_val = project_2_utils.load_data(n_features = 30)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Downloading data from https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\n","131072/124103 [===============================] - 0s 1us/step\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kwLCcUcowkIZ","colab_type":"text"},"source":["Train"]},{"cell_type":"code","metadata":{"id":"CBDaDzukwecg","colab_type":"code","colab":{}},"source":["batch_size = 16\n","learning_rate = 0.01\n","epochs = 50\n","layer_sizes = [30, 128, 1]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yDq1UD6mwnr2","colab_type":"code","outputId":"5e74f6fc-a432-4a93-afc1-0e8e22dd7869","executionInfo":{"status":"ok","timestamp":1586887365224,"user_tz":240,"elapsed":1189,"user":{"displayName":"Cihan Soylu","photoUrl":"https://lh4.googleusercontent.com/-i3-9q-bZNSc/AAAAAAAAAAI/AAAAAAAAAng/7qvvlFUHucw/s64/photo.jpg","userId":"04575478996568472015"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["avg_loss = []\n","val_losses = []\n","parameters = initialize_weights_and_biases(layer_sizes)\n","\n","num_steps = len(x_train) // batch_size\n","\n","for epoch in range(1,epochs+1):\n","    \n","    losses = []\n","    for step in range(0,num_steps):\n","        batch_x = x_train[step*batch_size: (step+1)*batch_size] \n","        batch_y = y_train[step*batch_size: (step+1)*batch_size] \n","\n","        y_hat, memory_list = forward_pass(batch_x, parameters)\n","        loss = compute_loss(y_hat, batch_y)\n","        losses.append(loss)\n","        gradients = backpropagate(memory_list, batch_y, y_hat)\n","\n","        parameters = update_parameters(gradients, parameters, learning_rate)\n","\n","\n","    avg_loss.append(np.average(losses))\n","            \n","    y_hat, memory_list = forward_pass(x_val, parameters)\n","    val_loss = compute_loss(y_hat, y_val)\n","    val_losses.append(val_loss)\n","    val_accuracy = accuracy(y_val, predict(x_val, parameters))\n","    \n","    print(\"epoch %i,   Validation Loss %f, Validation Accuracy: %f\" %(epoch, val_loss, val_accuracy))\n","        \n","plt.plot(val_losses, label = \"val_loss\")\n","plt.plot(avg_loss, label = \"loss\")\n","plt.ylabel('Loss')\n","plt.xlabel('Epochs')\n","plt.legend()\n","plt.show()"],"execution_count":13,"outputs":[{"output_type":"stream","text":["epoch 1,   Validation Loss 0.511796, Validation Accuracy: 0.894737\n","epoch 2,   Validation Loss 0.393157, Validation Accuracy: 0.935673\n","epoch 3,   Validation Loss 0.326848, Validation Accuracy: 0.947368\n","epoch 4,   Validation Loss 0.285226, Validation Accuracy: 0.935673\n","epoch 5,   Validation Loss 0.257006, Validation Accuracy: 0.935673\n","epoch 6,   Validation Loss 0.236611, Validation Accuracy: 0.941520\n","epoch 7,   Validation Loss 0.221114, Validation Accuracy: 0.941520\n","epoch 8,   Validation Loss 0.208942, Validation Accuracy: 0.941520\n","epoch 9,   Validation Loss 0.199067, Validation Accuracy: 0.941520\n","epoch 10,   Validation Loss 0.190875, Validation Accuracy: 0.941520\n","epoch 11,   Validation Loss 0.183950, Validation Accuracy: 0.941520\n","epoch 12,   Validation Loss 0.178002, Validation Accuracy: 0.941520\n","epoch 13,   Validation Loss 0.172818, Validation Accuracy: 0.947368\n","epoch 14,   Validation Loss 0.168247, Validation Accuracy: 0.953216\n","epoch 15,   Validation Loss 0.164179, Validation Accuracy: 0.953216\n","epoch 16,   Validation Loss 0.160539, Validation Accuracy: 0.953216\n","epoch 17,   Validation Loss 0.157252, Validation Accuracy: 0.959064\n","epoch 18,   Validation Loss 0.154262, Validation Accuracy: 0.959064\n","epoch 19,   Validation Loss 0.151542, Validation Accuracy: 0.959064\n","epoch 20,   Validation Loss 0.149043, Validation Accuracy: 0.959064\n","epoch 21,   Validation Loss 0.146737, Validation Accuracy: 0.959064\n","epoch 22,   Validation Loss 0.144609, Validation Accuracy: 0.959064\n","epoch 23,   Validation Loss 0.142638, Validation Accuracy: 0.959064\n","epoch 24,   Validation Loss 0.140807, Validation Accuracy: 0.953216\n","epoch 25,   Validation Loss 0.139096, Validation Accuracy: 0.953216\n","epoch 26,   Validation Loss 0.137500, Validation Accuracy: 0.953216\n","epoch 27,   Validation Loss 0.136006, Validation Accuracy: 0.953216\n","epoch 28,   Validation Loss 0.134602, Validation Accuracy: 0.959064\n","epoch 29,   Validation Loss 0.133285, Validation Accuracy: 0.959064\n","epoch 30,   Validation Loss 0.132051, Validation Accuracy: 0.959064\n","epoch 31,   Validation Loss 0.130890, Validation Accuracy: 0.959064\n","epoch 32,   Validation Loss 0.129798, Validation Accuracy: 0.959064\n","epoch 33,   Validation Loss 0.128767, Validation Accuracy: 0.959064\n","epoch 34,   Validation Loss 0.127790, Validation Accuracy: 0.959064\n","epoch 35,   Validation Loss 0.126872, Validation Accuracy: 0.959064\n","epoch 36,   Validation Loss 0.126002, Validation Accuracy: 0.959064\n","epoch 37,   Validation Loss 0.125178, Validation Accuracy: 0.959064\n","epoch 38,   Validation Loss 0.124395, Validation Accuracy: 0.959064\n","epoch 39,   Validation Loss 0.123653, Validation Accuracy: 0.959064\n","epoch 40,   Validation Loss 0.122949, Validation Accuracy: 0.964912\n","epoch 41,   Validation Loss 0.122284, Validation Accuracy: 0.964912\n","epoch 42,   Validation Loss 0.121655, Validation Accuracy: 0.964912\n","epoch 43,   Validation Loss 0.121058, Validation Accuracy: 0.959064\n","epoch 44,   Validation Loss 0.120497, Validation Accuracy: 0.959064\n","epoch 45,   Validation Loss 0.119964, Validation Accuracy: 0.959064\n","epoch 46,   Validation Loss 0.119460, Validation Accuracy: 0.959064\n","epoch 47,   Validation Loss 0.118980, Validation Accuracy: 0.959064\n","epoch 48,   Validation Loss 0.118525, Validation Accuracy: 0.959064\n","epoch 49,   Validation Loss 0.118096, Validation Accuracy: 0.959064\n","epoch 50,   Validation Loss 0.117687, Validation Accuracy: 0.959064\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"Ho4EiH6Jw5In","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}